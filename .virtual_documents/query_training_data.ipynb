!pip install langchain langchain-openai langchain-community faiss-cpu tiktoken 
!pip install python-dotenv


import os
import glob
import gc
import logging
from dotenv import load_dotenv
from langchain_openai import OpenAI
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings  # Ensure this import is here
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
#example of prompt template
from langchain_core.prompts import ChatPromptTemplate
#converts the chat model output into a string for convenience
from langchain_core.output_parsers import StrOutputParser

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Load environment variables from .env file
dotenv_path = os.path.join(os.getcwd(), '.env')
if not os.path.exists(dotenv_path):
    raise ValueError(f".env file not found at {dotenv_path}")

load_dotenv(dotenv_path)

# Get the API key
api_key = os.getenv('OPENAI_API_KEY')

# Verify that the API key is loaded correctly
if not api_key:
    raise ValueError("API key not found. Please ensure it is set in the .env file.")
else:
    logging.info("API key loaded successfully")

# Initialize OpenAI LLM
llm = OpenAI(api_key=api_key)

# Load all text files from the "training_data" directory
document_paths = glob.glob("training_data/*.txt")
documents = []

logging.info(f"Found {len(document_paths)} documents")

for path in document_paths:
    loader = TextLoader(path)
    documents.extend(loader.load())

logging.info(f"Loaded {len(documents)} documents")

gc.collect()

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=0,
    length_function=len,
)

docs = text_splitter.split_documents(documents)

logging.info(f"Split documents into {len(docs)} chunks")

gc.collect()

try:
    embedding = OpenAIEmbeddings(api_key=api_key)
except Exception as e:
    logging.error(f"Error initializing OpenAI embeddings: {e}")
    raise

# Proceed with FAISS index creation
batch_size = 100
batches = [docs[i:i + batch_size] for i in range(0, len(docs), batch_size)]

logging.info("Creating FAISS library in batches...")

try:
    for i, batch in enumerate(batches):
        logging.info(f"Processing batch {i + 1}/{len(batches)}")
        if i == 0:
            library = FAISS.from_documents(batch, embedding)
        else:
            batch_library = FAISS.from_documents(batch, embedding)
            library.merge_from(batch_library)
        del batch
        gc.collect()
except Exception as e:
    logging.error(f"Error during FAISS library creation: {e}")
    raise

logging.info("FAISS library created")

# Proceed with similarity search and QA steps incrementally
Query1 = "what makes Observe better than other observability platforms?"

try:
    Query_Answer = library.similarity_search(Query1)
    logging.info(f"Query result: {Query_Answer[0].page_content}")
except Exception as e:
    logging.error(f"Error during similarity search: {e}")
    raise

print(Query_Answer[1].page_content)

try:
    docs_and_scores = library.similarity_search_with_score(Query1)
except Exception as e:
    logging.error(f"Error during similarity search with score: {e}")
    raise

retriever = library.as_retriever()

try:
    qa = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=retriever)
except Exception as e:
    logging.error(f"Error creating RetrievalQA: {e}")
    raise

retriever_query = "what is the best Observability feature?"

try:
    results = qa.invoke(retriever_query)
    logging.info(f"Results: {results}")
except Exception as e:
    logging.error(f"Error during QA invocation: {e}")
    raise

# Save FAISS index
try:
    library.save_local("faiss_index_observe")
except Exception as e:
    logging.error(f"Error saving FAISS index: {e}")
    raise

try:
    observe_saved = FAISS.load_local("faiss_index_observe", embedding, allow_dangerous_deserialization=True)
except Exception as e:
    logging.error(f"Error loading FAISS index: {e}")
    raise

try:
    qa = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=observe_saved.as_retriever())
except Exception as e:
    logging.error(f"Error creating RetrievalQA with saved index: {e}")
    raise

try:
    results = qa.invoke(retriever_query)
    logging.info(f"Results after loading FAISS index: {results}")
except Exception as e:
    logging.error(f"Error during QA invocation with saved index: {e}")
    raise




