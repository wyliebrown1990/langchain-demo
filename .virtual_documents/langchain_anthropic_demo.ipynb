import os
from datetime import date
from langchain_anthropic import ChatAnthropic
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import HumanMessage
from langchain_core.output_parsers import StrOutputParser #responsible for parsing the output of a model into a more accessible format
from langchain.globals import set_debug
from dotenv import load_dotenv

set_debug(True)

# Load environment variables from .env file
load_dotenv()

# Access the API key
api_key = os.getenv('ANTHROPIC_API_KEY')

#initialize model
chat_model = ChatAnthropic(
    model="claude-3-sonnet-20240229",
    temperature=0
)

#chat model uses chat messages as input/output. So resulting message: AIMessage(content="...") looks like that. 
chat_model.invoke([
    HumanMessage("Tell me a joke about bears!")
])

#construct a prompt template consisting of templates for a SystemMessage and a HumanMessage using from_messages. 
#SystemMessages are meta-instructions that are not part of the current conversation, but purely guide input.
#
joke_prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a world class comedian."),
    ("human", "Tell me a joke about {topic}") #The prompt template contains {topic} in curly braces. This denotes a required parameter named "topic"
])

#You can compose Runnables into “chains” using the pipe (|) operator where you .invoke() the next step with the output of the previous one.

chain = joke_prompt | chat_model
joke_prompt.invoke({"topic": "beets"}) #You invoke the prompt template with a dict with a key named "topic" and a value "beets"
chain.invoke({"topic": "beets"})

"""If you want to work with the raw string output of the of the message:
LangChain has a component called an Output Parser, which, as the name implies, is responsible for parsing the output of a model into a more accessible format. 
Since composed chains are also Runnable, you can again use the pipe operator:
"""
str_chain = chain | StrOutputParser()
str_chain.invoke({"topic": "beets"})

#You can iterate over the output using for ... in syntax. Try it with the str_chain you just declared:
for chunk in str_chain.stream({"topic": "beets"}):
    print(chunk, end="|")

chat_model = ChatAnthropic(model_name="claude-3-sonnet-20240229")

prompt = ChatPromptTemplate.from_messages([
    ("system", 'You know that the current date is "{current_date}".'),
    ("human", "{question}")
])

chain = prompt | chat_model | StrOutputParser()

chain.invoke({
    "question": "What is the current date?",
    "current_date": date.today()
})

#Simple RAG example:

chat_model = ChatAnthropic(model_name="claude-3-sonnet-20240229")

SOURCE = """
Old Ship Saloon 2023 quarterly revenue numbers:
Q1: $174782.38
Q2: $467372.38
Q3: $474773.38
Q4: $389289.23
"""

rag_prompt = ChatPromptTemplate.from_messages([
    ("system", 'You are a helpful assistant. Use the following context when responding:\n\n{context}.'),
    ("human", "{question}")
])

rag_chain = rag_prompt | chat_model | StrOutputParser()

rag_chain.invoke({
    "question": "What was the Old Ship Saloon's total revenue in Q1 2023?",
    "context": SOURCE
})



#Simple RAG example:

chat_model = ChatAnthropic(model_name="claude-3-sonnet-20240229")

SOURCE = """
Old Ship Saloon 2023 quarterly revenue numbers:
Q1: $174782.38
Q2: $467372.38
Q3: $474773.38
Q4: $389289.23
"""

rag_prompt = ChatPromptTemplate.from_messages([
    ("system", 'You are a helpful assistant. Use the following context when responding:\n\n{context}.'),
    ("human", "{question}")
])

rag_chain = rag_prompt | chat_model | StrOutputParser()

rag_chain.invoke({
    "question": "What was the Old Ship Saloon's total revenue in Q1 2023?",
    "context": SOURCE
})



